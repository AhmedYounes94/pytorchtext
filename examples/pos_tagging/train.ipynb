{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import fasttext\n",
    "import sys\n",
    "from collections import namedtuple, defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "from embedding import EmbeddingReader\n",
    "from crf.crf import ConditionalRandomField\n",
    "from lstm import LSTMEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: CONLL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(train_sentences):\n",
    "    vocab = namedtuple('vocab', ['word2idx', 'idx2word'])\n",
    "    vocab.word2idx, vocab.idx2word = dict(), dict()\n",
    "    vocab.word2idx[\"<oov>\"] = 0\n",
    "    vocab.word2idx[\"<pad>\"] = 1\n",
    "    for sent in train_sentences:\n",
    "        for word_tup in sent:\n",
    "            word = word_tup[0]\n",
    "            if word not in vocab.word2idx:\n",
    "                vocab.word2idx[word] = len(vocab.word2idx)\n",
    "    vocab.word2idx = defaultdict(lambda : vocab.word2idx[\"<oov>\"], vocab.word2idx)\n",
    "    vocab.idx2word = {v:k for k, v in vocab.word2idx.items()}\n",
    "    return vocab\n",
    "\n",
    "def build_tagmap(train_sentences):\n",
    "    tagmap = namedtuple('tagmap', ['tag2idx', 'idx2tag'])\n",
    "    tagmap.tag2idx, tagmap.idx2tag = dict(), dict()\n",
    "    for sent in train_sentences:\n",
    "        for word_tup in sent:\n",
    "            tag = word_tup[2]\n",
    "            if tag not in tagmap.tag2idx:\n",
    "                tagmap.tag2idx[tag] = len(tagmap.tag2idx)\n",
    "    tagmap.idx2tag = {v:k for k,v in tagmap.tag2idx.items()}\n",
    "    return tagmap\n",
    "                \n",
    "\n",
    "def pad_sequence(lst_of_lsts, token):\n",
    "    max_length = max(len(x) for x in lst_of_lsts)\n",
    "    result = []\n",
    "    for lst in lst_of_lsts:\n",
    "        result.append(lst + [token] * (max_length - len(lst)))\n",
    "    return result\n",
    "\n",
    "def get_words_and_tags(batch, vocab, tagmap):\n",
    "    batch_sent, batch_tags = [], []\n",
    "    for sent in batch:\n",
    "        words, tags = [], []\n",
    "        for word_tup in sent:\n",
    "            words.append(vocab.word2idx[word_tup[0]])\n",
    "            tags.append(tagmap.tag2idx[word_tup[2]])\n",
    "        batch_sent.append(words)\n",
    "        batch_tags.append(tags)\n",
    "    return batch_sent, batch_tags\n",
    "\n",
    "def get_batch(sentences, vocab, tagmap, batch_size):\n",
    "    num_sentences, i = len(sentences), 0\n",
    "    while i < num_sentences:\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        i += batch_size\n",
    "        batch_sent, batch_tags = get_words_and_tags(batch, vocab, tagmap)\n",
    "        batch_sent = pad_sequence(batch_sent, vocab.word2idx[\"<pad>\"])\n",
    "        batch_tags = pad_sequence(batch_tags, 0)\n",
    "        yield batch_sent, batch_tags\n",
    "\n",
    "def train_model(model, dataset, num_epochs, learning_rate, vocab, tagmap, batch_size):\n",
    "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for batch_sent, batch_tags in get_batch(dataset, vocab, tagmap, batch_size):\n",
    "            torch_batch_sent, torch_batch_tags = torch.LongTensor(batch_sent).to(device), torch.LongTensor(batch_tags).to(device)\n",
    "            output = model(torch_batch_sent, torch_batch_tags)\n",
    "            output[\"loss\"].backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(output[\"loss\"].item())\n",
    "        print(f\"Epoch {epoch} is complete, Avg Loss = {np.mean(losses)}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, dataset, vocab, tagmap, batch_size):\n",
    "    model.eval()\n",
    "    def unpack_sequence_to_instance(prediction, gnd_list, tagmap, mask):\n",
    "        pred_list = prediction.tolist()\n",
    "        mask = mask.tolist()\n",
    "        pred_tags, gnd_tags = [], []\n",
    "        for i in range(len(mask)):\n",
    "            for j in range(len(mask[0])):\n",
    "                if mask[i][j] == 1:\n",
    "                    pred_tags.append(tagmap.idx2tag[pred_list[i][j]])\n",
    "                    gnd_tags.append(tagmap.idx2tag[gnd_list[i][j]])\n",
    "            \n",
    "        assert len(pred_tags) == len(gnd_tags)\n",
    "        return pred_tags, gnd_tags\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        y_pred, y_gnd = [], []\n",
    "        for batch_sent, batch_tags in get_batch(dataset, vocab, tagmap, batch_size):\n",
    "            torch_batch_sent, torch_batch_tags = torch.LongTensor(batch_sent).to(device), torch.LongTensor(batch_tags).to(device)\n",
    "            mask = torch_batch_sent != vocab.word2idx[\"<pad>\"]\n",
    "            output = model(torch_batch_sent, torch_batch_tags)\n",
    "            predictions = output[\"out_sequence\"].argmax(1)\n",
    "            pred, gnd = unpack_sequence_to_instance(predictions, batch_tags, tagmap, mask)\n",
    "            y_pred += pred\n",
    "            y_gnd += gnd\n",
    "        model.train()\n",
    "        return classification_report(y_pred, y_gnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_dim,\n",
    "                 num_layers,\n",
    "                 batch_first,\n",
    "                 dropout,\n",
    "                 num_directions,\n",
    "                 device,\n",
    "                 embedding_dim,\n",
    "                 embedding_path,\n",
    "                 vocab,\n",
    "                 target_map):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        embedding_reader = EmbeddingReader(embedding_dim,\n",
    "                                           embedding_path,\n",
    "                                           vocab)\n",
    "        embedding_matrix = embedding_reader.get_embedding_matrix()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.lstm_encoder = LSTMEncoder(input_size,\n",
    "                                        hidden_dim,\n",
    "                                        num_layers,\n",
    "                                        batch_first,\n",
    "                                        dropout,\n",
    "                                        bidirectional= True if num_directions == 2 else False,\n",
    "                                        device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        n_class = len(target_map.tag2idx)\n",
    "        self.out = nn.Linear(num_directions * hidden_dim, n_class)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        self.vocab = vocab\n",
    "\n",
    "        \n",
    "    def forward(self, sentences, targets):\n",
    "        mask = (sentences != self.vocab.word2idx[\"<pad>\"]).float()\n",
    "        lengths = torch.sum(mask, dim=1)\n",
    "        embedding = self.embedding(sentences)\n",
    "        dropped_embeddings = self.dropout(embedding)\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(dropped_embeddings,\n",
    "                                                                lengths,\n",
    "                                                                batch_first=True,\n",
    "                                                                enforce_sorted=False)\n",
    "        output, (h_n, c_n) = self.lstm_encoder(packed_embeddings, lengths.shape[0])\n",
    "        unpacked_output, lengths = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        out = self.out(unpacked_output)\n",
    "        out = out.transpose(2, 1)\n",
    "        output = {\"loss\": self.loss_fn(out, targets, mask), \"out_sequence\": out}\n",
    "        return output\n",
    "\n",
    "    def loss_fn(self, predicted, target, mask):\n",
    "        loss = self.criterion(predicted, target)\n",
    "        loss = (loss * mask).sum()\n",
    "        nnz = (mask != 0).sum()\n",
    "        return loss / nnz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM CRF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the CRF model in the init method.\n",
    "\n",
    "```\n",
    "        self.crf = ConditionalRandomField(n_class, label_encoding=\"BIO\", idx2tag=tagmap.idx2tag)\n",
    "          \n",
    "```\n",
    "\n",
    "We compute log likelihood of the CRF model in the forward pass.\n",
    "\n",
    "```\n",
    "        log_likelihood = self.crf(logits, target, mask)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCRFModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_dim,\n",
    "                 num_layers,\n",
    "                 batch_first,\n",
    "                 dropout,\n",
    "                 num_directions,\n",
    "                 device,\n",
    "                 embedding_dim,\n",
    "                 embedding_path,\n",
    "                 vocab,\n",
    "                 target_map):\n",
    "        super(LSTMCRFModel, self).__init__()\n",
    "        embedding_reader = EmbeddingReader(embedding_dim,\n",
    "                                           embedding_path,\n",
    "                                           vocab)\n",
    "        embedding_matrix = embedding_reader.get_embedding_matrix()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.lstm_encoder = LSTMEncoder(input_size,\n",
    "                                        hidden_dim,\n",
    "                                        num_layers,\n",
    "                                        batch_first,\n",
    "                                        dropout,\n",
    "                                        bidirectional= True if num_directions == 2 else False,\n",
    "                                        device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        n_class = len(target_map.tag2idx)\n",
    "        self.out = nn.Linear(num_directions * hidden_dim, n_class)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        self.vocab = vocab\n",
    "        self.crf = ConditionalRandomField(n_class, label_encoding=\"BIO\", idx2tag=tagmap.idx2tag)\n",
    "\n",
    "        \n",
    "    def forward(self, sentences, targets=None):\n",
    "        mask = (sentences != self.vocab.word2idx[\"<pad>\"]).int()\n",
    "        lengths = torch.sum(mask, dim=1)\n",
    "        embedding = self.embedding(sentences)\n",
    "        dropped_embeddings = self.dropout(embedding)\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(dropped_embeddings,\n",
    "                                                                lengths,\n",
    "                                                                batch_first=True,\n",
    "                                                                enforce_sorted=False)\n",
    "        output, (h_n, c_n) = self.lstm_encoder(packed_embeddings, lengths.shape[0])\n",
    "        unpacked_output, lengths = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        out = self.out(unpacked_output)\n",
    "        \n",
    "        best_tag_sequence = self.crf.best_viterbi_tag(out, mask)\n",
    "        \n",
    "        class_probabilities = out * 0.0\n",
    "        for i, instance_tags in enumerate(best_tag_sequence):\n",
    "            for j, tag_id in enumerate(instance_tags[0][0]):\n",
    "                class_probabilities[i, j, int(tag_id)] = 1        \n",
    "\n",
    "        output = {\"out_sequence\": class_probabilities.transpose(2, 1)}\n",
    "        \n",
    "        \n",
    "        if targets is not None:\n",
    "            output[\"loss\"] = self.loss_fn(out, targets, mask)\n",
    "        return output\n",
    "\n",
    "    def loss_fn(self, logits, target, mask):\n",
    "        log_likelihood = self.crf(logits, target, mask)\n",
    "        return -log_likelihood / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(train_sents)\n",
    "tagmap = build_tagmap(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "embedding_dim = 300\n",
    "input_size = embedding_dim\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "learning_rate = 0.01\n",
    "batch_first = True\n",
    "dropout = 0.1\n",
    "num_directions = 2\n",
    "num_epochs = 10\n",
    "embedding_path = \"/users/talurj/Downloads/cc.en.300.bin\" # Path to fasttext embedding\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMModel(input_size,\n",
    "                       hidden_dim,\n",
    "                       num_layers,\n",
    "                       batch_first,\n",
    "                       dropout,\n",
    "                       num_directions,\n",
    "                       device,\n",
    "                       embedding_dim,\n",
    "                       embedding_path,\n",
    "                       vocab,\n",
    "                       tagmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 is complete, Avg Loss = 0.3515281883921203\n",
      "Epoch 1 is complete, Avg Loss = 0.1767707201455973\n",
      "Epoch 2 is complete, Avg Loss = 0.1331593554780972\n",
      "Epoch 3 is complete, Avg Loss = 0.10572671983391047\n",
      "Epoch 4 is complete, Avg Loss = 0.08466701770479652\n",
      "Epoch 5 is complete, Avg Loss = 0.07157210325572454\n",
      "Epoch 6 is complete, Avg Loss = 0.06412825188367588\n",
      "Epoch 7 is complete, Avg Loss = 0.05590231993977851\n",
      "Epoch 8 is complete, Avg Loss = 0.050978807256096735\n",
      "Epoch 9 is complete, Avg Loss = 0.04405360433390771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.63      0.86      0.73       784\n",
      "      B-MISC       0.57      0.63      0.60       307\n",
      "       B-ORG       0.77      0.83      0.79      1295\n",
      "       B-PER       0.72      0.89      0.80       593\n",
      "       I-LOC       0.55      0.78      0.65       229\n",
      "      I-MISC       0.60      0.70      0.64       476\n",
      "       I-ORG       0.66      0.92      0.77       796\n",
      "       I-PER       0.77      0.97      0.86       506\n",
      "           O       1.00      0.97      0.98     46547\n",
      "\n",
      "    accuracy                           0.96     51533\n",
      "   macro avg       0.70      0.84      0.76     51533\n",
      "weighted avg       0.97      0.96      0.96     51533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = train_model(lstm_model, train_sents, num_epochs, learning_rate, vocab, tagmap, batch_size)\n",
    "print(evaluate_model(lstm_model, test_sents, vocab, tagmap, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_crf_model = LSTMCRFModel(input_size,\n",
    "                       hidden_dim,\n",
    "                       num_layers,\n",
    "                       batch_first,\n",
    "                       dropout,\n",
    "                       num_directions,\n",
    "                       device,\n",
    "                       embedding_dim,\n",
    "                       embedding_path,\n",
    "                       vocab,\n",
    "                       tagmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_crf_model = train_model(lstm_crf_model, train_sents, num_epochs, learning_rate, vocab, tagmap, batch_size)\n",
    "print(evaluate_model(lstm_crf_model, test_sents, vocab, tagmap, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that LSTM-CRF model improves macro avg f1 from 0.73 to 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
